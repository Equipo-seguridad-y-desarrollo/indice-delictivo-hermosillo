# ðŸ“‹ DocumentaciÃ³n del Proceso de Limpieza de Datos
## Proyecto: Ãndice Delictivo Hermosillo

**Ãšltima actualizaciÃ³n**: 10 de noviembre de 2025

---

## ðŸ“Š Resumen del Proyecto

Este documento describe el proceso completo de descarga, limpieza, normalizaciÃ³n, estandarizaciÃ³n y enriquecimiento de datos geogrÃ¡ficos para el anÃ¡lisis del Ã­ndice delictivo en Hermosillo, Sonora (2018-2025).

**Cambios importantes en v3.0**:
- âœ… Nueva Fase 0: Descarga y procesamiento de shapefile INE_Limpio
- âœ… Script colonias_poligonos.py para obtener polÃ­gonos desde fuente pÃºblica
- âœ… Trazabilidad completa de la fuente de datos geogrÃ¡ficos
- âœ… Pipeline automatizado incluye descarga de shapefiles

**Cambios importantes en v2.0**:
- âœ… MigraciÃ³n de Google Drive a Hugging Face para descarga de datos
- âœ… Pipeline consolidado para procesamiento multi-aÃ±o (2.3M registros)
- âœ… EstandarizaciÃ³n de 475 tipos de incidentes a 198 categorÃ­as
- âœ… Feature engineering: 7 columnas derivadas (temporal, categÃ³rica, severidad)
- âœ… GeocodificaciÃ³n incremental para optimizar costos de API
- âœ… OptimizaciÃ³n de esquema (10 columnas esenciales)

---

## ðŸ—‚ï¸ Estructura de Datos

### Archivos de Entrada (Raw Data)
```
data/raw/
â”œâ”€â”€ 213.xlsx                         # Datos de incidentes 911 (8 hojas: 2018-2025)
â”œâ”€â”€ reportes_de_incidentes_2018_2025.csv  # Consolidado de Excel
â”œâ”€â”€ delitos.csv                      # CatÃ¡logo de tipos de delitos
â”œâ”€â”€ INE_Limpio.shp (+ .dbf, .shx, .prj)  # Shapefile de colonias (descargado)
â”œâ”€â”€ demografia_hermosillo.csv        # Datos demogrÃ¡ficos por colonia
â””â”€â”€ poligonos_hermosillo.csv         # PolÃ­gonos geogrÃ¡ficos (generado desde shapefile)
```

### Archivos Intermedios (Interim Data)
```
data/interim/
â””â”€â”€ reportes_de_incidentes_procesados_2018_2025.csv  # 2.3M registros procesados (~310MB)
```

### Archivos Generados (Processed Data)
```
data/processed/
â”œâ”€â”€ colonias_unicas_reportes_911.csv                # 2,047 colonias Ãºnicas
â”œâ”€â”€ colonias_reportes_911_agrupadas_reporte.csv    # Reporte de 220 grupos con variantes
â”œâ”€â”€ mapeo_colonias_reportes_911.csv                # Mapeo de 2,296 variantes â†’ 2,047 Ãºnicas
â”œâ”€â”€ colonias_reportes_911_con_coordenadas.csv      # Colonias con lat/lng (geocodificadas)
â”œâ”€â”€ demografia_limpio.csv                          # DemografÃ­a con espacios normalizados
â””â”€â”€ colonias_unicas_demografia.csv                 # 659 colonias Ãºnicas de demografÃ­a
```

---

## ðŸ”„ Flujo del Proceso

### **Fase 0: PreparaciÃ³n de PolÃ­gonos GeogrÃ¡ficos**

#### 0.1 Script: `colonias_poligonos.py`

**Objetivo**: Descargar y procesar el shapefile oficial de colonias de Sonora para extraer los polÃ­gonos de Hermosillo

**Nueva fuente de datos**:
- **Repositorio**: [ColoniasSonora](https://github.com/Sonora-en-Datos/ColoniasSonora) de Luis Moreno
- **Archivo**: `INE_Limpio.shp` (Marco GeoestadÃ­stico Nacional del INE)
- **Alcance**: Todo el estado de Sonora
- **Calidad**: Datos oficiales verificados por el INE

**Proceso**:
```python
# 1. Descarga automÃ¡tica de archivos del shapefile
repo_url = "https://github.com/Sonora-en-Datos/ColoniasSonora/raw/main/shapes/INE_Limpio/"
files = ["INE_Limpio.shp", "INE_Limpio.dbf", "INE_Limpio.shx", "INE_Limpio.prj"]

# 2. Cargar shapefile completo
gdf_completo = gpd.read_file("INE_Limpio.shp")

# 3. Filtrar geometrÃ­as vÃ¡lidas (Polygon + MultiPolygon)
gdf_poligonos = gdf_completo[gdf_completo.geometry.type.isin(['Polygon', 'MultiPolygon'])]

# 4. Filtrar solo Hermosillo
gdf_hermosillo = gdf_poligonos[gdf_poligonos['nom_loc'] == 'Hermosillo']

# 5. Exportar a CSV
gdf_hermosillo.to_csv('data/raw/poligonos_hermosillo.csv', index=False)
```

**Resultados**:
- **Registros totales del shapefile**: Varios miles (todo Sonora)
- **Colonias de Hermosillo extraÃ­das**: ~700
- **Tipos de geometrÃ­a incluidos**: 
  - Polygon: Colonias con Ã¡rea continua
  - MultiPolygon: Colonias con Ã¡reas discontinuas (no son errores)
- **Archivo generado**: `data/raw/poligonos_hermosillo.csv`

**Por quÃ© MultiPolygon no es un error**:
Algunas colonias tienen Ã¡reas geogrÃ¡ficas separadas por infraestructura (avenidas, vÃ­as de tren, etc.) pero mantienen el mismo nombre administrativo. Ejemplos comunes:
- Colonias divididas por avenidas principales
- Fraccionamientos con secciones separadas
- Colonias con parques o Ã¡reas pÃºblicas intermedias

**InformaciÃ³n obtenida**:
- GeometrÃ­as (polÃ­gonos en formato WKT)
- Claves geogrÃ¡ficas (CVE_COL, CVE_ENT, CVE_MUN)
- Nombres oficiales de colonias
- CÃ³digos postales
- Datos del Ãndice de MarginaciÃ³n 2020 (CONAPO)
- Indicadores de carencias sociales

---

### **Fase 1: Descarga y ConsolidaciÃ³n de Datos**

#### 1.1 Script: `download_raw_data.py`

**Objetivo**: Descargar datos desde Hugging Face y consolidar Excel multi-hoja en CSV Ãºnico

**MigraciÃ³n realizada**:
- **Antes**: Google Drive API con autenticaciÃ³n OAuth2
- **DespuÃ©s**: Descarga directa HTTP desde Hugging Face
- **Beneficio**: Sin autenticaciÃ³n, mÃ¡s simple, mÃ¡s confiable

**Proceso**:
```python
# 1. Descarga desde Hugging Face
url = "https://huggingface.co/datasets/Marcelinux/llamadas911_colonias_hermosillo_2018_2025/resolve/main/213.xlsx"
response = requests.get(url, stream=True)

# 2. Lectura multi-hoja
all_sheets = pd.read_excel(BytesIO(response.content), sheet_name=None)

# 3. ExtracciÃ³n de aÃ±o desde nombre de hoja
for sheet_name, df_sheet in all_sheets.items():
    year = int(sheet_name)  # "2018" â†’ 2018
    df_sheet['AÃ±o_Reporte'] = year

# 4. ConsolidaciÃ³n
df_consolidated = pd.concat(list_dfs, ignore_index=True)
```

**Resultados**:
- **Hojas procesadas**: 8 (2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025)
- **Registros totales**: 2,297,081
- **Columnas originales**: COLONIA, TIPO DE INCIDENTE, FECHA, HORA
- **Columna aÃ±adida**: AÃ±o_Reporte
- **Archivo generado**: `data/raw/reportes_de_incidentes_2018_2025.csv`

---

### **Fase 2: Procesamiento Interim - EstandarizaciÃ³n y Feature Engineering**

#### 2.1 Script: `make_interim_data.py`

**Objetivo**: Estandarizar tipos de incidentes, categorizar, generar features temporales y optimizar esquema

**Componentes del procesamiento**:

##### A. EstandarizaciÃ³n de Tipos de Incidentes
```python
MAPA_DE_INCIDENTES = {
    # 475 reglas de mapeo
    "PORTACION DE ARMAS O CARTUCHOS": "PORTACIÃ“N DE ARMAS O CARTUCHOS",
    "PERSONA AGRESIVA": "PERSONA AGRESIVA",
    "APOYO A LA CIUDADANIA": "APOYO A LA CIUDADANÃA",
    # ... 472 reglas mÃ¡s
}

# Aplicar normalizaciÃ³n
df['TIPO DE INCIDENTE'] = df['TIPO DE INCIDENTE'].map(MAPA_DE_INCIDENTES)
```

**Resultados estandarizaciÃ³n**:
- **Tipos originales**: 475 variantes
- **Tipos Ãºnicos post-mapeo**: 198
- **Registros sin mapeo**: 7 (mantienen valor original)
- **ReducciÃ³n**: 58% en variabilidad

##### B. CategorizaciÃ³n de Incidentes
```python
CATEGORIAS_INCIDENTES = {
    # 216 reglas de categorizaciÃ³n en 12 grupos
    "PORTACIÃ“N DE ARMAS O CARTUCHOS": "Armas y Objetos Peligrosos",
    "PERSONA AGRESIVA": "Violencia y AgresiÃ³n",
    "APOYO A LA CIUDADANÃA": "Apoyo Ciudadano",
    # ... 213 reglas mÃ¡s
}

df['Categoria_Incidente'] = df['TIPO DE INCIDENTE'].map(CATEGORIAS_INCIDENTES).fillna('Otros')
```

**12 CategorÃ­as principales**:
1. Violencia y AgresiÃ³n
2. TrÃ¡nsito y VehÃ­culos
3. Apoyo Ciudadano
4. Delitos Patrimoniales
5. AlteraciÃ³n del Orden
6. Sospechosos y Vigilancia
7. Menores y Familia
8. Armas y Objetos Peligrosos
9. Emergencias MÃ©dicas
10. FenÃ³menos Naturales
11. Espacios PÃºblicos
12. Otros

##### C. ClasificaciÃ³n de Severidad
```python
NIVEL_SEVERIDAD = {
    # 200 reglas de clasificaciÃ³n
    "PORTACIÃ“N DE ARMAS O CARTUCHOS": "ALTA",
    "PERSONA AGRESIVA": "MEDIA",
    "APOYO A LA CIUDADANÃA": "BAJA",
    # ... 197 reglas mÃ¡s
}

df['Nivel_Severidad'] = df['TIPO DE INCIDENTE'].map(NIVEL_SEVERIDAD).fillna('MEDIA')
```

**3 Niveles de severidad**:
- **ALTA**: Incidentes graves (armas, agresiÃ³n violenta, allanamiento)
- **MEDIA**: Incidentes moderados (persona agresiva, vehÃ­culo sospechoso)
- **BAJA**: Incidentes leves (apoyo ciudadano, animales en vÃ­a pÃºblica)

##### D. Feature Engineering Temporal
```python
# 1. Timestamp consolidado
df['Timestamp'] = pd.to_datetime(df['FECHA'] + ' ' + df['HORA'].astype(str) + ':00:00')

# 2. Parte del dÃ­a (binning de horas)
df['ParteDelDia'] = pd.cut(
    df['Timestamp'].dt.hour, 
    bins=[-1, 5, 11, 17, 23], 
    labels=['Madrugada', 'MaÃ±ana', 'Tarde', 'Noche']
)

# 3. DÃ­a de la semana
dias_map = {0:'Lunes', 1:'Martes', 2:'MiÃ©rcoles', 3:'Jueves', 
            4:'Viernes', 5:'SÃ¡bado', 6:'Domingo'}
df['DiaDeLaSemana'] = df['Timestamp'].dt.dayofweek.map(dias_map)

# 4. Fin de semana
df['EsFinDeSemana'] = df['Timestamp'].dt.dayofweek.isin([5, 6]).map({True: 'SÃ­', False: 'No'})

# 5. Mes
df['Mes'] = df['Timestamp'].dt.month

# 6. Quincena (dÃ­as de pago tÃ­picos)
dias_quincena = [1, 14, 15, 16, 28, 29, 30, 31]
df['EsQuincena'] = df['Timestamp'].dt.day.isin(dias_quincena).map({True: 'SÃ­', False: 'No'})
```

##### E. OptimizaciÃ³n de Esquema
```python
# Columnas redundantes eliminadas: FECHA, HORA, AÃ±o_Reporte
# Solo se mantiene Timestamp como referencia temporal Ãºnica

final_cols = [
    'COLONIA',
    'TIPO DE INCIDENTE',
    'Timestamp',
    'ParteDelDia',
    'DiaDeLaSemana',
    'EsFinDeSemana',
    'Mes',
    'EsQuincena',
    'Categoria_Incidente',
    'Nivel_Severidad'
]

df_final = df[final_cols]
```

**OptimizaciÃ³n lograda**:
- **Antes**: 13 columnas (COLONIA, TIPO, FECHA, HORA, AÃ±o, Timestamp, + 7 derivadas)
- **DespuÃ©s**: 10 columnas (eliminadas FECHA, HORA, AÃ±o_Reporte redundantes)
- **Beneficio**: -23% tamaÃ±o, menos confusiÃ³n temporal

**Archivo generado**:
- **Ruta**: `data/interim/reportes_de_incidentes_procesados_2018_2025.csv`
- **TamaÃ±o**: ~310 MB
- **Registros**: 2,297,081
- **Periodo**: 2018-01-01 00:00:00 a 2025-09-30 23:00:00
- **Encoding**: UTF-8 con BOM (utf-8-sig)

---

### **Fase 3: Limpieza de Datos Policiales - ExtracciÃ³n de Colonias**

#### 3.1 MigraciÃ³n del Script

**Cambio importante**: `extraer_colonias_unicas_reportes_911.py` migrado para usar datos procesados del interim

- **Antes**: Usaba `data/raw/213.csv` (obsoleto)
- **DespuÃ©s**: Usa `data/interim/reportes_de_incidentes_procesados_2018_2025.csv`
- **Beneficio**: Opera sobre datos ya estandarizados y enriquecidos

#### 3.2 AnÃ¡lisis Inicial
- **Archivo**: `reportes_de_incidentes_procesados_2018_2025.csv`
- **Registros totales**: 2,297,081
- **Colonias originales**: 2,296
- **Problema identificado**: MÃºltiples errores ortogrÃ¡ficos y variantes del mismo nombre

#### 3.3 Algoritmo de NormalizaciÃ³n

**Objetivo**: Identificar y agrupar colonias con errores ortogrÃ¡ficos usando fuzzy matching

**Algoritmo implementado**:

```python
# 1. NormalizaciÃ³n de texto
def normalizar_texto(texto):
    - Convertir a MAYÃšSCULAS
    - Remover acentos
    - Normalizar espacios mÃºltiples
    
# 2. CÃ¡lculo de similitud
- Usar SequenceMatcher de difflib
- Umbral: 90% de similitud

# 3. ValidaciÃ³n de variantes
def son_variantes_validas():
    âœ“ Detectar nÃºmeros romanos diferentes (VI â‰  VIII)
    âœ“ Detectar nÃºmeros arÃ¡bigos diferentes (1 â‰  2)
    âœ“ Detectar sectores/etapas diferentes
    âœ“ Detectar nombres distintivos diferentes
    âœ“ Solo agrupar errores ortogrÃ¡ficos reales
```

**Reglas de agrupaciÃ³n**:
1. **SÃ agrupar**:
   - Diferencias solo en acentos: `JESÃšS` â†” `JESUS`
   - Errores tipogrÃ¡ficos: `PUERTA` â†” `PUERAT`
   - Espacios inconsistentes: `LOS OLIVOS` â†” `LO OLIVOS`

2. **NO agrupar**:
   - NÃºmeros diferentes: `LAS PEREDAS` â‰  `LAS PEREDAS 2`
   - NÃºmeros romanos: `PUERTA REAL VI` â‰  `PUERTA REAL VIII`
   - Sectores diferentes: `SOLIDARIDAD IV` â‰  `SOLIDARIDAD V`
   - Nombres distintivos: `PINOS` â‰  `ENCINOS`

**Resultados**:
- **Colonias Ãºnicas finales**: 2,047
- **Grupos con variantes**: 220
- **Registros mapeados**: 2,296
- **Variante representativa**: La mÃ¡s frecuente (asume que la mayorÃ­a escribe correctamente)

**Ejemplo de agrupaciÃ³n exitosa**:
```
'QUINTA ESMERALDA' (1,511 registros)
  - QUINTA ESMELRALDA (1)    â† Error tipogrÃ¡fico
  - QUINTA ESMERAL (1)       â† Nombre incompleto
  - QUINTA ESMERALDA (1,508) â† âœ“ Forma correcta (mÃ¡s frecuente)
  - QUINTA ESMERALDA| (1)    â† CarÃ¡cter extra
```

**Archivos generados**:
- `colonias_unicas_reportes_911.csv`: Lista de 2,047 colonias limpias
- `colonias_reportes_911_agrupadas_reporte.csv`: Reporte detallado de 220 grupos con variantes
- `mapeo_colonias_reportes_911.csv`: Mapeo de cada una de las 2,296 colonias originales a su versiÃ³n normalizada

---

###**Fase 4: GeocodificaciÃ³n con Google Maps API**

#### 4.1 Script: `geocodificar_colonias_reportes_911.py`

**Objetivo**: Obtener coordenadas geogrÃ¡ficas (latitud/longitud) para cada colonia con sistema incremental

**Mejora implementada**: **GeocodificaciÃ³n Incremental**

**Antes (v1.0)**:
- Geocodificaba todas las colonias en cada ejecuciÃ³n
- Costo: ~$6 USD por ejecuciÃ³n completa
- Tiempo: ~8-10 minutos
- Problema: Re-procesar colonias ya geocodificadas desperdicia tiempo y dinero

**DespuÃ©s (v2.0)**:
- Detecta automÃ¡ticamente colonias ya geocodificadas
- Solo procesa colonias nuevas
- **1era ejecuciÃ³n**: Geocodifica 2,047 colonias (~8-10 min, ~$6 USD)
- **Ejecuciones posteriores**: Solo colonias nuevas (segundos, $0.00)
- Combina geocodificaciones previas con nuevas en archivo Ãºnico

**LÃ³gica incremental**:
```python
# 1. Verificar si existe archivo de salida
if os.path.exists(archivo_salida):
    df_previas = pd.read_csv(archivo_salida)
    colonias_ya_geocodificadas = set(df_previas['COLONIA'].unique())
    
    # 2. Filtrar solo colonias nuevas
    df_colonias = df_colonias[~df_colonias['COLONIA'].isin(colonias_ya_geocodificadas)]
    
    if len(df_colonias) == 0:
        print("[OK] Todas las colonias ya estÃ¡n geocodificadas")
        return df_previas

# 3. Geocodificar solo las nuevas
# ... proceso de geocodificaciÃ³n ...

# 4. Combinar previas + nuevas
df_resultados = pd.concat([df_previas, df_nuevas], ignore_index=True)
```

**ConfiguraciÃ³n de seguridad**:
```python
# âœ“ API Key en variable de entorno (.env)
GOOGLE_MAPS_API_KEY=tu_api_key_aqui

# âœ“ ProtecciÃ³n con .gitignore
.env  # Nunca subir al repositorio
```

**ParÃ¡metros de geocodificaciÃ³n**:
```python
direccion = f"{colonia}, Hermosillo, Sonora, MÃ©xico"
delay = 0.2  # segundos entre peticiones (evitar lÃ­mites de API)
```

**Resultados**:
- **Colonias procesadas**: 2,047
- **Tiempo aproximado**: 8-10 minutos (primera ejecuciÃ³n)
- **Tasa de Ã©xito**: ~100% (todas encontradas)
- **Costo estimado inicial**: ~$6.34 USD (incluido en crÃ©dito gratuito de $200/mes)
- **Ejecuciones posteriores**: Solo colonias nuevas (ahorro significativo)

**InformaciÃ³n obtenida por colonia**:
```csv
COLONIA, LATITUD, LONGITUD, DIRECCION_FORMATEADA, TIPO_UBICACION, PLACE_ID, TIPOS, TIMESTAMP
```

**Ejemplo**:
```csv
QUINTA ESMERALDA,29.075595,-110.957462,"Quinta Esmeralda, 83000 Hermosillo, Son., Mexico",APPROXIMATE,ChIJ...,political|sublocality,2025-11-06T15:30:45
```

**Archivo generado**:
- `colonias_reportes_911_con_coordenadas.csv`: 2,047 colonias con coordenadas y metadata

---

### **Fase 5: Limpieza de Datos DemogrÃ¡ficos**

#### 5.1 AnÃ¡lisis: `analizar_calidad_datos_demografia.py`

**Objetivo**: Verificar calidad de datos demogrÃ¡ficos

**Resultados del anÃ¡lisis**:
- **Registros totales**: 660
- **Colonias Ãºnicas**: 660
- **Calidad de datos**: âœ“ Excelente (casi sin errores)

**Variantes detectadas inicialmente**: 11 grupos
- MayorÃ­a eran colonias genuinamente diferentes
- Solo 1 error real detectado: `PRIMERO  HERMOSILLO` (doble espacio)

#### 5.2 Script: `normalizar_espacios_demografia.py`

**Objetivo**: Normalizar solo errores obvios (espacios dobles)

**Proceso**:
```python
def normalizar_espacios(texto):
    return ' '.join(texto.split())
```

**Resultados**:
- **Correcciones aplicadas**: 2 registros
  - `PRIMERO  HERMOSILLO` â†’ `PRIMERO HERMOSILLO`
  - `LA CORUÃ‘A SECCION  PRIVADA ALMAR` â†’ `LA CORUÃ‘A SECCION PRIVADA ALMAR`
- **Colonias finales**: 659 (consolidÃ³ 1 duplicado)

**Archivos generados**:
- `demografia_limpio.csv`: Dataset limpio
- `colonias_unicas_demografia.csv`: 659 colonias Ãºnicas

---

## ðŸ“ˆ MÃ©tricas del Proceso

### Pipeline Completo

| Fase | Input | Output | Tiempo | Costo |
|------|-------|--------|--------|-------|
| 0. PolÃ­gonos | GitHub shapefile | 700 polÃ­gonos CSV | ~1 min | $0 |
| 1. Descarga | Hugging Face | 2.3M registros CSV | ~2 min | $0 |
| 2. Procesamiento Interim | Raw CSV | Procesado con 10 cols | ~5 min | $0 |
| 3. ExtracciÃ³n Colonias | Procesado | 2,047 colonias Ãºnicas | ~30 seg | $0 |
| 4. GeocodificaciÃ³n (1era) | Colonias Ãºnicas | Con coordenadas | ~8-10 min | ~$6 |
| 4. GeocodificaciÃ³n (subsec.) | Solo nuevas | Incremental | segundos | ~$0 |

### Datos Policiales (2018-2025)
| MÃ©trica | Antes | DespuÃ©s | Mejora |
|---------|-------|---------|--------|
| Registros totales | 2,297,081 | 2,297,081 | - |
| Colonias Ãºnicas | 2,296 | 2,047 | -249 (-10.8%) |
| Tipos de incidentes | 475 | 198 | -277 (-58.3%) |
| Columnas | 4â†’13 | 10 | Optimizado |
| Errores detectados | 220 grupos | 0 | 100% normalizado |
| Variantes por grupo | Hasta 4 | - | Consolidadas |

### EstandarizaciÃ³n y Enriquecimiento
| MÃ©trica | Valor |
|---------|-------|
| Tipos estandarizados | 475 â†’ 198 |
| CategorÃ­as creadas | 12 |
| Niveles de severidad | 3 (BAJA, MEDIA, ALTA) |
| Features temporales aÃ±adidas | 5 |
| Features categÃ³ricas aÃ±adidas | 2 |
| Periodo de datos | 2018-01-01 a 2025-09-30 |

### Datos DemogrÃ¡ficos
| MÃ©trica | Antes | DespuÃ©s | Mejora |
|---------|-------|---------|--------|
| Colonias Ãºnicas | 660 | 659 | -1 (1 duplicado) |
| Errores detectados | 1 | 0 | 100% limpio |

### GeocodificaciÃ³n
| MÃ©trica | Valor |
|---------|-------|
| Colonias geocodificadas | 2,047 |
| Tasa de Ã©xito | ~100% |
| Tiempo (1era ejecuciÃ³n) | ~8-10 min |
| Costo (1era ejecuciÃ³n) | ~$6.34 USD |
| Tiempo (ejecuciones posteriores) | segundos |
| Costo (ejecuciones posteriores) | $0.00 USD |

---

## ðŸ”§ Scripts Desarrollados

### Scripts de Pipeline Principal

0. **`colonias_poligonos.py`**
   - Descarga automÃ¡tica del shapefile INE_Limpio desde GitHub
   - Filtrado de geometrÃ­as vÃ¡lidas (Polygon + MultiPolygon)
   - ExtracciÃ³n de colonias de Hermosillo (~700 registros)
   - ExportaciÃ³n a CSV para integraciÃ³n con pipeline

1. **`indice_delictivo_hermosillo_main.py`**
   - Orquestador del pipeline completo
   - Ejecuta descarga â†’ procesamiento interim
   - Manejo de errores y logging

2. **`download_raw_data.py`**
   - Descarga desde Hugging Face
   - ConsolidaciÃ³n de Excel multi-hoja
   - ExtracciÃ³n de aÃ±os desde nombres de hojas

3. **`make_interim_data.py`**
   - EstandarizaciÃ³n de 475 tipos de incidentes
   - CategorizaciÃ³n en 12 grupos principales
   - ClasificaciÃ³n de severidad (3 niveles)
   - Feature engineering temporal (5 features)
   - OptimizaciÃ³n de esquema (10 columnas)

### Scripts de Procesamiento de Colonias

5. **`extraer_colonias_unicas_reportes_911.py`**
   - Limpieza y normalizaciÃ³n de nombres de colonias
   - Algoritmo de fuzzy matching (90% umbral)
   - ValidaciÃ³n inteligente de variantes
   - Migrado para usar datos del interim

5. **`geocodificar_colonias_reportes_911.py`**
   - GeocodificaciÃ³n con Google Maps API
   - **Sistema incremental anti-duplicados** (v2.0)
   - Manejo seguro de credenciales
   - Delay entre peticiones (0.2s)

### Scripts de AnÃ¡lisis

6. **`normalizar_espacios_demografia.py`**
   - NormalizaciÃ³n de espacios en datos demogrÃ¡ficos
   - Proceso minimalista (solo errores obvios)

7. **`analizar_calidad_datos_demografia.py`**
   - AnÃ¡lisis de calidad de datos demogrÃ¡ficos
   - DetecciÃ³n de posibles duplicados

---

## ðŸ›¡ï¸ Seguridad

### ProtecciÃ³n de Credenciales
```bash
# Archivo .env (NO SUBIR A GIT)
GOOGLE_MAPS_API_KEY=tu_api_key_aqui

# .gitignore
.env
.venv/
```

### Buenas PrÃ¡cticas Implementadas
âœ“ API keys en variables de entorno  
âœ“ ValidaciÃ³n de existencia de variables  
âœ“ Delay entre peticiones de API  
âœ“ Manejo de errores robusto  
âœ“ DocumentaciÃ³n de seguridad (SECURITY.md)

---

## ðŸ“ Nomenclatura y Convenciones

### Nombres de Variables
```python
# âœ“ Descriptivos y en espaÃ±ol
colonias_unicas        # Lista de colonias sin duplicados
frecuencias           # Diccionario {colonia: conteo}
umbral_similitud      # Float: 0.90

# âœ“ Funciones verbos en infinitivo
normalizar_texto()
obtener_coordenadas()
son_variantes_validas()
```

### Nombres de Archivos
```
# Scripts - PatrÃ³n: {acciÃ³n}_{objeto}_{fuente}.py
extraer_colonias_unicas_reportes_911.py
geocodificar_colonias_reportes_911.py
normalizar_espacios_demografia.py
analizar_calidad_datos_demografia.py

# Datos procesados - PatrÃ³n: {objeto}_{fuente}_{detalle}.csv
colonias_unicas_reportes_911.csv
colonias_reportes_911_con_coordenadas.csv
mapeo_colonias_reportes_911.csv
colonias_unicas_demografia.csv
demografia_limpio.csv
```

---

## ðŸŽ¯ PrÃ³ximos Pasos Sugeridos

1. **ValidaciÃ³n Cruzada**
   - Comparar colonias entre reportes procesados y demografÃ­a
   - Identificar colonias faltantes en cada dataset
   - AnÃ¡lisis de cobertura geogrÃ¡fica

2. **Enriquecimiento de Datos**
   - Crear dataset maestro unificado con:
     * Reportes procesados (2.3M registros)
     * Coordenadas geogrÃ¡ficas (2,047 colonias)
     * Datos demogrÃ¡ficos (659 colonias)
     * PolÃ­gonos geogrÃ¡ficos
   - Calcular mÃ©tricas agregadas por colonia

3. **AnÃ¡lisis Temporal**
   - Explotar features temporales (ParteDelDia, DiaDeLaSemana, EsQuincena)
   - Identificar patrones estacionales
   - AnÃ¡lisis de tendencias 2018-2025

4. **AnÃ¡lisis por CategorÃ­a y Severidad**
   - Mapas de calor por nivel de severidad
   - DistribuciÃ³n de categorÃ­as por colonia
   - IdentificaciÃ³n de zonas crÃ­ticas

5. **AnÃ¡lisis Geoespacial**
   - Mapear incidentes delictivos por colonia
   - AnÃ¡lisis de densidad delictiva
   - Clusters espaciales (hotspots)
   - CorrelaciÃ³n espacial con Ã­ndice de marginaciÃ³n

6. **VisualizaciÃ³n**
   - Crear mapas interactivos (Folium, Plotly)
   - Dashboards con mÃ©tricas por colonia
   - Timeline de incidentes
   - Heatmaps por categorÃ­a y hora del dÃ­a

---

## ðŸ“š Dependencias

```bash
# Python packages
pandas>=2.0.0          # ManipulaciÃ³n de datos
googlemaps>=4.10.0     # GeocodificaciÃ³n
python-dotenv>=1.0.0   # Variables de entorno
requests>=2.31.0       # Descarga HTTP
openpyxl>=3.1.0        # Lectura de Excel

# API Services
Google Maps Geocoding API
Hugging Face Datasets
```

---

## ðŸ‘¥ Equipo

**Equipo-seguridad-y-desarrollo**  
**Proyecto**: indice-delictivo-hermosillo  
**Rama actual**: colonias_geolocalizadas_unificadas

---

*Ãšltima actualizaciÃ³n: 10 de noviembre de 2025*
