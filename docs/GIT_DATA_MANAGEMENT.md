# üóÇÔ∏è Mejores Pr√°cticas para Manejo de Datos en Git

## ‚ö†Ô∏è REGLA DE ORO
**Nunca subas archivos de datos grandes (>10MB) directamente a Git**

Git no est√° dise√±ado para archivos grandes. Cada cambio duplica el archivo en el historial, haciendo el repositorio pesado e inmanejable.

---

## üìã ESTRATEGIAS PROFESIONALES

### 1. **Git + .gitignore (Proyectos Peque√±os/Medianos)**
‚úÖ **Mejor para**: Proyectos con datos < 100MB, equipos peque√±os

#### Configuraci√≥n `.gitignore`
```gitignore
# Excluir archivos grandes de datos procesados
/data/raw/*.csv
/data/interim/*.csv
/data/processed/**/*.csv
/data/processed/**/*.geojson

# Permitir archivos peque√±os de metadatos
!/data/**/.gitkeep
!/data/**/README.md

# Excluir visualizaciones pesadas
*.html
mapa_*.html
```

#### Estructura recomendada:
```
data/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ .gitkeep          ‚úÖ Incluir en Git
‚îÇ   ‚îú‚îÄ‚îÄ README.md         ‚úÖ Incluir (documenta fuentes)
‚îÇ   ‚îî‚îÄ‚îÄ datos.csv         ‚ùå Excluir (archivo grande)
‚îú‚îÄ‚îÄ interim/
‚îÇ   ‚îú‚îÄ‚îÄ .gitkeep          ‚úÖ Incluir
‚îÇ   ‚îî‚îÄ‚îÄ procesado.csv     ‚ùå Excluir
‚îî‚îÄ‚îÄ processed/
    ‚îú‚îÄ‚îÄ .gitkeep          ‚úÖ Incluir
    ‚îî‚îÄ‚îÄ final.csv         ‚ùå Excluir
```

#### Scripts para reproducibilidad:
```python
# download_raw_data.py - ‚úÖ INCLUIR EN GIT
"""
Script para descargar datos desde fuente original
"""
import requests
from pathlib import Path

def download_data():
    url = "https://huggingface.co/datasets/..."
    output = Path("data/raw/datos.csv")
    # ... c√≥digo de descarga
```

---

### 2. **Git LFS (Large File Storage)**
‚úÖ **Mejor para**: Archivos binarios grandes (modelos ML, im√°genes, datos < 2GB)

#### Instalaci√≥n:
```bash
# Instalar Git LFS
git lfs install

# Trackear tipos de archivo espec√≠ficos
git lfs track "*.csv"
git lfs track "*.parquet"
git lfs track "*.h5"
git lfs track "*.pkl"

# Esto crea/actualiza .gitattributes
```

#### Ventajas:
- Los archivos grandes se almacenan en servidor externo
- Git solo guarda punteros (~100 bytes)
- Clonaci√≥n m√°s r√°pida

#### Desventajas:
- **GitHub LFS**: 1GB gratis, luego $5/mes por 50GB
- **GitLab LFS**: 10GB gratis
- Requiere configuraci√≥n adicional

#### Ejemplo `.gitattributes`:
```
*.csv filter=lfs diff=lfs merge=lfs -text
*.parquet filter=lfs diff=lfs merge=lfs -text
*.h5 filter=lfs diff=lfs merge=lfs -text
```

---

### 3. **DVC (Data Version Control)** üåü
‚úÖ **Mejor para**: Proyectos de ML/Data Science profesionales, pipelines reproducibles

#### Instalaci√≥n:
```bash
pip install dvc
dvc init

# Configurar almacenamiento remoto
dvc remote add -d myremote s3://mybucket/dvc-storage
# o Google Drive, Azure, SSH, etc.
```

#### Uso b√°sico:
```bash
# Trackear archivo de datos
dvc add data/raw/datos.csv
# Esto crea data/raw/datos.csv.dvc (ESTE s√≠ va a Git)

# Subir datos al storage remoto
dvc push

# Otro miembro del equipo puede obtener los datos
git clone <repo>
dvc pull  # Descarga los datos desde storage remoto
```

#### Pipeline reproducible:
```yaml
# dvc.yaml - Define tu pipeline
stages:
  download:
    cmd: python download_raw_data.py
    outs:
      - data/raw/reportes.csv
  
  process:
    cmd: python process_data.py
    deps:
      - data/raw/reportes.csv
    outs:
      - data/processed/clean_data.csv
  
  train:
    cmd: python train_model.py
    deps:
      - data/processed/clean_data.csv
    outs:
      - models/model.pkl
```

```bash
# Ejecutar todo el pipeline
dvc repro

# Ver diferencias en datos entre commits
dvc diff
```

#### Ventajas:
- Versionamiento de datos como si fuera c√≥digo
- Pipelines reproducibles
- Soporta m√∫ltiples backends (S3, GCS, Azure, Drive, SSH)
- Gratuito y open-source
- Maneja archivos enormes (TB+)

#### Desventajas:
- Curva de aprendizaje
- Requiere configurar storage remoto

---

### 4. **Cloud Storage + Scripts de Descarga**
‚úÖ **Mejor para**: Datos p√∫blicos, datasets enormes (GB-TB)

#### Fuentes comunes:
- **Hugging Face Datasets** (tu caso actual) ‚úÖ
- AWS S3 (con requester-pays o bucket p√∫blico)
- Google Cloud Storage
- Azure Blob Storage
- Kaggle Datasets

#### Ejemplo con Hugging Face:
```python
# download_raw_data.py
from datasets import load_dataset
from pathlib import Path

def download_hermosillo_data():
    """Descarga datos desde Hugging Face"""
    dataset = load_dataset(
        "Equipo-seguridad-y-desarrollo/hermosillo-incidentes",
        split="train"
    )
    
    # Convertir a DataFrame y guardar
    df = dataset.to_pandas()
    output_path = Path("data/raw/reportes_de_incidentes_2018_2025.csv")
    df.to_csv(output_path, index=False)
    print(f"‚úì Datos descargados: {output_path}")
    print(f"  Registros: {len(df):,}")

if __name__ == "__main__":
    download_hermosillo_data()
```

#### README.md con instrucciones:
```markdown
## Obtener los datos

Los datos NO est√°n incluidos en el repositorio. Para obtenerlos:

1. Instala dependencias: `pip install -r requirements.txt`
2. Ejecuta: `python notebooks/download_raw_data.py`
3. Los datos se descargar√°n en `data/raw/`

**Fuente**: Hugging Face - Equipo-seguridad-y-desarrollo/hermosillo-incidentes
**Tama√±o**: ~500MB (2.3M registros)
**Actualizaci√≥n**: Septiembre 2025
```

---

### 5. **Database + SQL Dumps**
‚úÖ **Mejor para**: Datos estructurados, m√∫ltiples usuarios, consultas complejas

#### Setup:
```bash
# PostgreSQL local
pg_dump mydb > data/raw/dump.sql      # ‚ùå No incluir dump completo

# Solo esquema en Git
pg_dump --schema-only mydb > schema.sql  # ‚úÖ Incluir esquema
```

#### Alternativa con seeds:
```python
# seed_database.py
import pandas as pd
from sqlalchemy import create_engine

def seed_from_cloud():
    """Carga datos desde cloud a DB local"""
    df = pd.read_csv("https://storage.url/data.csv")
    
    engine = create_engine('postgresql://localhost/mydb')
    df.to_sql('reportes', engine, if_exists='replace')
```

---

## üéØ RECOMENDACI√ìN PARA TU PROYECTO

### Situaci√≥n Actual:
- Datos: 2.3M registros (~500MB CSV)
- Fuente: Hugging Face (p√∫blico)
- Equipo: Peque√±o
- Repositorio: GitHub

### ‚úÖ Estrategia Recomendada: **Cloud Storage + Scripts**

#### Por qu√©:
1. **Ya tienes los datos en Hugging Face** ‚úÖ
2. **Datos son p√∫blicos** - cualquiera puede descargarlos
3. **No pagas por storage adicional** (HF es gratis)
4. **Reproducible** - `download_raw_data.py` automatiza la descarga
5. **Simple** - no requiere configuraci√≥n compleja

#### Implementaci√≥n:
```bash
# Ya lo tienes implementado:
notebooks/
‚îú‚îÄ‚îÄ download_raw_data.py          ‚úÖ En Git
‚îú‚îÄ‚îÄ process_data.py               ‚úÖ En Git
‚îú‚îÄ‚îÄ unificar_datos_poligonos.py   ‚úÖ En Git
‚îî‚îÄ‚îÄ ...

data/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ .gitkeep                  ‚úÖ En Git
‚îÇ   ‚îú‚îÄ‚îÄ README.md                 ‚úÖ En Git (documenta fuentes)
‚îÇ   ‚îî‚îÄ‚îÄ *.csv                     ‚ùå Excluido (.gitignore)
‚îî‚îÄ‚îÄ processed/
    ‚îú‚îÄ‚îÄ .gitkeep                  ‚úÖ En Git
    ‚îî‚îÄ‚îÄ *.csv                     ‚ùå Excluido (.gitignore)

.gitignore                        ‚úÖ Configurado correctamente
requirements.txt                  ‚úÖ En Git
```

#### Para colaboradores:
```bash
# 1. Clonar repo
git clone https://github.com/Equipo-seguridad-y-desarrollo/indice-delictivo-hermosillo.git

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Descargar datos
python notebooks/download_raw_data.py

# 4. Ejecutar pipeline
python notebooks/unificar_datos_poligonos.py
python notebooks/mapa_interactivo_folium_avanzado.py

# 5. Abrir mapa
# mapa_interactivo_hermosillo.html
```

---

## üìä COMPARACI√ìN DE ESTRATEGIAS

| Estrategia | Costo | Complejidad | Tama√±o Max | Mejor para |
|-----------|-------|------------|-----------|-----------|
| **.gitignore** | Gratis | ‚≠ê Baja | - | Datos generables |
| **Git LFS** | $5+/mes | ‚≠ê‚≠ê Media | 2GB | Binarios (modelos) |
| **DVC** | Variable | ‚≠ê‚≠ê‚≠ê Alta | Ilimitado | ML profesional |
| **Cloud + Scripts** | Gratis* | ‚≠ê Baja | Ilimitado | Datos p√∫blicos |
| **Database** | Variable | ‚≠ê‚≠ê‚≠ê Alta | Ilimitado | Datos relacionales |

*Asumiendo storage gratuito o existente

---

## üöÄ EVOLUCI√ìN DEL PROYECTO

### Actual (v4.0): Cloud + Scripts ‚úÖ
- Perfecto para tu etapa actual
- Colaboraci√≥n simple
- Costo cero

### Futuro (si escala):
1. **Datos privados/sensibles** ‚Üí DVC + S3 privado
2. **Actualizaciones frecuentes** ‚Üí Database + API
3. **Modelos ML grandes** ‚Üí Git LFS para modelos
4. **Pipeline complejo** ‚Üí DVC pipelines

---

## üìù CHECKLIST DE MEJORES PR√ÅCTICAS

### ‚úÖ Implementado en tu proyecto:
- [x] `.gitignore` excluye archivos grandes
- [x] Scripts de descarga versionados
- [x] Pipeline de procesamiento reproducible
- [x] Documentaci√≥n de fuentes de datos
- [x] Estructura de directorios est√°ndar

### üîÑ Opcional para mejorar:
- [ ] Crear `data/raw/README.md` documentando fuentes
- [ ] Agregar checksums/hashes para validar descargas
- [ ] Implementar tests de calidad de datos
- [ ] Configurar CI/CD para validar pipeline
- [ ] Documentar tama√±os esperados de archivos

---

## üîó RECURSOS ADICIONALES

### Documentaci√≥n:
- **Git LFS**: https://git-lfs.github.com/
- **DVC**: https://dvc.org/doc
- **Hugging Face Datasets**: https://huggingface.co/docs/datasets

### Art√≠culos:
- [How to Version Control Large Files](https://dagshub.com/blog/version-control-large-files/)
- [Managing ML Projects with DVC](https://realpython.com/python-data-version-control/)

### Alternativas:
- **Delta Lake** (datos tipo data warehouse)
- **LakeFS** (Git para data lakes)
- **Pachyderm** (data versioning + pipelines)

---

## üí° RESUMEN EJECUTIVO

### Para tu proyecto Hermosillo:
1. **Mant√©n el enfoque actual**: Cloud (Hugging Face) + Scripts
2. **No subas CSVs grandes a Git** - ya est√° bien configurado
3. **Version√° scripts, no datos** - exactamente lo que hiciste
4. **Documenta fuentes** - agrega README en data/raw/
5. **Si crece el equipo/datos** - considera DVC

### Filosof√≠a clave:
> "Git versiona el C√ìDIGO que genera los datos, no los datos mismos"

Los datos son **artefactos reproducibles**, no c√≥digo fuente.

---

**√öltima actualizaci√≥n**: 7 de noviembre de 2025  
**Versi√≥n del proyecto**: v4.0  
**Estrategia actual**: Cloud Storage + Scripts de descarga
